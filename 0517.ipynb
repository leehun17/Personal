{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# í”„ë¡œì íŠ¸ëª…: ë¶ˆê· í˜• ë°ì´í„° ì²˜ë¦¬ ì‹¤ìŠµ (SMOTE ì˜¤ë²„ìƒ˜í”Œë§)\n",
    "\n",
    "\n",
    "\n",
    "![ì˜¤ë²„ìƒ˜í”Œë§](https://file.notion.so/f/f/4e4750b9-2973-47f2-9fe5-8330ce1f2c19/42b23d5f-2fb5-4922-b654-664749c9756b/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA_2024-05-16_%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE_9.40.53.png?id=8b3171b4-e83a-4cc2-b8de-207d2beaced3&table=block&spaceId=4e4750b9-2973-47f2-9fe5-8330ce1f2c19&expirationTimestamp=1715954400000&signature=HDwzwK1g1Rx1Wyyd5xr-WNNG-rRxsNJApTltz1D8ddw&downloadName=%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA+2024-05-16+%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE+9.40.53.png)\n",
    "\n",
    "#### ğŸ“Œ SMOTE (Synthetic Minority Over-sampling Technique) ë°©ë²•\n",
    "- ì†Œìˆ˜ì¸ ë°ì´í„°ì— Kê·¼ì ‘ ì´ì›ƒì„ ì°¾ì•„ì„œ, ë¬´ì‘ìœ„ë¡œ ì´ì›ƒì„ ì„ íƒí•©ë‹ˆë‹¤.\n",
    "- ê·¸ë¦¬ê³  ì„ íƒí•œ ì´ì›ƒ ì‚¬ì´ì— ê°€ìƒì˜ ë°ì´í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤. \n",
    "\n",
    "\n",
    "#### ğŸš¨ ì£¼ì˜ì‚¬í•­\n",
    "- SMOTEë¥¼ ì ìš©í•œë‹¤ê³  ë¬´ì¡°ê±´ ëª¨ë¸ì˜ ì„±ëŠ¥ì´ ì¢‹ì•„ì§€ëŠ” ê²Œ ì•„ë‹™ë‹ˆë‹¤.\n",
    "- ì •ìƒ: 99.9% / ë¹„ì •ìƒ 0.1% ì²˜ëŸ¼ ê·¹íˆ ë¶ˆê· í˜•í•œ ë°ì´í„°ì…‹ì— ì ìš©í•˜ì§€ë§Œ, ë…¸ì´ì¦ˆê°€ ìˆëŠ” ë°ì´í„°ë¼ë©´ ì˜¤íˆë ¤ ë…¸ì´ì¦ˆê°€ ì¦ê°€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "![SMOTE](https://file.notion.so/f/f/4e4750b9-2973-47f2-9fe5-8330ce1f2c19/8c10a2a3-affb-4117-b568-2c1e787f8521/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA_2024-05-16_%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE_9.58.47.png?id=4e3faf5e-2d23-4fea-b0ea-f64228590407&table=block&spaceId=4e4750b9-2973-47f2-9fe5-8330ce1f2c19&expirationTimestamp=1715954400000&signature=du8RYq2_Y66LG0jb3Bv7t1rI5bbtCLf21sH_h5swKdM&downloadName=%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA+2024-05-16+%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE+9.58.47.png)\n",
    "\n",
    "#### ì‹¤ìŠµ ë°ì´í„° : ì‹ ìš©ì¹´ë“œ ì‚¬ê¸° ë°ì´í„° \n",
    "\n",
    "- ì •ìƒ í´ë˜ìŠ¤ : 99.8%\n",
    "- ë¹„ì •ìƒ í´ë˜ìŠ¤ : 0.02%\n",
    "- Amount : ê¸ˆì•¡ (ìœ ë¡œí™”)\n",
    "- Class : ì‚¬ê¸° ì—¬ë¶€ (ì •ìƒ:0, ì‚¬ê¸°:1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ë¶„ì„ ê³¼ì •\n",
    "\n",
    "1. ì¤‘ìš” ë³€ìˆ˜ì˜ ë¶„í¬ë„ ë³€ê²½ (ë¦¬ìŠ¤ì¼€ì¼)\n",
    "> - âœ… Log ë³€í™˜: Skew(ì™œë„)ê°€ ì‹¬í•œ ë¶„í¬ë¥¼ `ì •ê·œë¶„í¬`ì²˜ëŸ¼ ë°”ê¿”ì£¼ëŠ” ë³€í™˜ ë°©ë²•\n",
    "> - âœ… ì •ê·œë¶„í¬ë¡œ ë°”ê¾¸ëŠ” ì´ìœ  : ëŒ€ë¶€ë¶„ MLì•Œê³ ë¦¬ì¦˜ì€ ì •ê·œë¶„í¬ì¼ ë•Œ ì„±ëŠ¥ì´ ì¢‹ì•„ì§€ê¸° ë•Œë¬¸ (íŠ¸ë¦¬ê¸°ë°˜ ì•Œê³ ë¦¬ì¦˜ì—ëŠ” íš¨ê³¼ ë¯¸ë¯¸)\n",
    "2. ì´ìƒì¹˜ ì œê±°\n",
    "> - âœ… ì´ìƒì¹˜ê°€ ìˆìœ¼ë©´ ëª¨ë¸ ì„±ëŠ¥ì´ ì €í•˜ë  ìˆ˜ ìˆìŒ (ì•ˆ ë  ìˆ˜ë„ ìˆê³ )\n",
    "3. SMOTE ì˜¤ë²„ ìƒ˜í”Œë§\n",
    "> - âœ… ì‚¬ê¸° ë°ì´í„° ë»¥íŠ€ê¸° ì‹œí‚´\n",
    "4. ëª¨ë¸ í•™ìŠµ ë° ì„±ëŠ¥ ë¹„êµ\n",
    "> - ë¡œì§€ìŠ¤í‹± íšŒê·€ëª¨ë¸, LightGBM ëŒ€ìƒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from lightgbm import LGBMClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('creditcard.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Class'].value_counts(), data['Class'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. ê³„ì†ì‚¬ìš©í•  ì½”ë“œë¥¼ í•¨ìˆ˜ë¡œ ìƒì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1. ì „ì²˜ë¦¬ í•¨ìˆ˜ (ì•ìœ¼ë¡œ ê³„ì† ë¡œì§ì´ ì¶”ê°€ë  ì˜ˆì •)\n",
    "def clean_data(df):\n",
    "    df_copy = df.copy()\n",
    "    df_copy = df_copy.drop(['Time'], axis=1)\n",
    "    return df_copy\n",
    "\n",
    "## 2. ë°ì´í„° ë¶„ë¦¬ í•¨ìˆ˜\n",
    "def get_train_test_dataset(df):\n",
    "    # ì¸ìë¡œ ì…ë ¥ëœ DataFrameì˜ ì‚¬ì „ ë°ì´í„° ê°€ê³µì´ ì™„ë£Œëœ ë³µì‚¬ DataFrame ë°˜í™˜\n",
    "    df_copy = clean_data(df)\n",
    "    # DataFrameì˜ ë§¨ ë§ˆì§€ë§‰ ì»¬ëŸ¼ì´ ì¢…ì†ë³€ìˆ˜, ë‚˜ë¨¸ì§€ëŠ” ë…ë¦½ë³€ìˆ˜\n",
    "    X_features = df_copy.iloc[:, :-1]\n",
    "    y_target = df_copy.iloc[:, -1]\n",
    "\n",
    "    ## train_test_split()ìœ¼ë¡œ í•™ìŠµê³¼ í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¶„í• .\n",
    "    ## stratify=y_targetìœ¼ë¡œ Stratified ê¸°ë°˜ ë¶„í• .\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_features, y_target, test_size=0.3, random_state=0, stratify=y_target)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "## 3. ëª¨ë¸ì„±ëŠ¥ í‰ê°€ í•¨ìˆ˜\n",
    "def get_clf_eval(y_test, pred=None, pred_proba=None):\n",
    "    confusion = confusion_matrix( y_test, pred)\n",
    "    accuracy = accuracy_score(y_test , pred)\n",
    "    precision = precision_score(y_test , pred)\n",
    "    recall = recall_score(y_test , pred)\n",
    "    f1 = f1_score(y_test,pred)\n",
    "    roc_auc = roc_auc_score(y_test, pred_proba)\n",
    "\n",
    "    print('========= ì˜¤ì°¨ í–‰ë ¬ ========= ')\n",
    "    print(confusion)\n",
    "    print('=========================== ')\n",
    "\n",
    "    print('ì •í™•ë„: {0:.4f}, ì •ë°€ë„: {1:.4f}, ì¬í˜„ìœ¨: {2:.4f},\\\n",
    "    â­ï¸F1: {3:.4f}, AUC:{4:.4f}'.format(accuracy, precision, recall, f1, roc_auc))\n",
    "\n",
    "## 4. ëª¨ë¸ í•™ìŠµ ë° í‰ê°€ í•¨ìˆ˜\n",
    "def get_model_train_eval(model, X_feature_train=None, X_feature_test=None, Y_feature_train=None, Y_feature_test=None):\n",
    "    model.fit(X_feature_train, Y_feature_train)\n",
    "    pred = model.predict(X_feature_test)\n",
    "    pred_proba = model.predict_proba(X_feature_test)[:, 1]\n",
    "    get_clf_eval(Y_feature_test, pred, pred_proba)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. í•™ìŠµë°ì´í„°ì™€ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì˜ ì¢…ì†ë³€ìˆ˜ ë¹„ìœ¨ í™•ì¸í•˜ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = get_train_test_dataset(data)\n",
    "\n",
    "print('í•™ìŠµ ë°ì´í„° ë ˆì´ë¸” ê°’ ë¹„ìœ¨')\n",
    "print(y_train.value_counts(normalize=True))\n",
    "print('í…ŒìŠ¤íŠ¸ ë°ì´í„° ë ˆì´ë¸” ê°’ ë¹„ìœ¨')\n",
    "print(y_test.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. ë¡œì§€ìŠ¤í‹± íšŒê·€ëª¨ë¸ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr_clf = LogisticRegression()\n",
    "lr_clf.fit(X_train, y_train)\n",
    "lr_pred = lr_clf.predict(X_test)\n",
    "lr_pred_proba = lr_clf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# 3ì¥ì—ì„œ ì‚¬ìš©í•œ get_clf_eval() í•¨ìˆ˜ë¥¼ ì´ìš©í•˜ì—¬ í‰ê°€ ìˆ˜í–‰. \n",
    "get_clf_eval(y_test, lr_pred, lr_pred_proba)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. LightGBM ëª¨ë¸\n",
    "##### ğŸš¨ ë¶ˆê· í˜• ë°ì´í„°ë¥¼ LightGBM ì‚¬ìš©ì‹œ, `boost_from_average=False`ë¥¼ ë¬´ì¡°ê±´ í•´ì¤˜ì•¼í•©ë‹ˆë‹¤.\n",
    "- ì•ˆ ê·¸ëŸ¬ë©´, ì„±ëŠ¥ì´ ì €í•˜ë©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_clf = LGBMClassifier(n_estimators=1000, \n",
    "                          num_leaves=64, \n",
    "                          n_jobs=-1, \n",
    "                          boost_from_average=False\n",
    "                          )\n",
    "\n",
    "get_model_train_eval(lgbm_clf, X_feature_train=X_train, X_feature_test=X_test, Y_feature_train=y_train, Y_feature_test=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. ì¤‘ìš” ë³€ìˆ˜ì¸ Amount ë³€ìˆ˜ì˜ ë¶„í¬ ë³€ê²½ í›„ ëª¨ë¸ ì„±ëŠ¥ í‰ê°€\n",
    "##### Amount ë¶„í¬ í™•ì¸\n",
    "\n",
    "##### â­ï¸ TIP. ë§¤ì¶œì•¡, ì†Œë“, ê¸ˆì•¡ ê°™ì€ ë³€ìˆ˜ë“¤ì€ ë³´í†µ Skew(ì™œë„)ê°€ ì‹¬í•œ ê²½ìš°ê°€ ë§ìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.xticks(range(0, 30000, 1000), rotation=60)\n",
    "sns.histplot(data['Amount'], bins=100, kde=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ê²½í—˜ì ìœ¼ë¡œ Skewê°€ ì‹¬í•œ ë°ì´í„°ëŠ” StandardScaler ë³´ë‹¤ Log ìŠ¤ì¼€ì¼ì´ ì¢€ ë” ì„±ëŠ¥ì´ ì¢‹ìŠµë‹ˆë‹¤. \n",
    "### 6. ì „ì²˜ë¦¬ í•¨ìˆ˜ì— StandardScaler ì¶”ê°€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "# ì‚¬ì´í‚·ëŸ°ì˜ StandardScalerë¥¼ ì´ìš©í•˜ì—¬ ì •ê·œë¶„í¬ í˜•íƒœë¡œ Amount í”¼ì²˜ê°’ ë³€í™˜í•˜ëŠ” ë¡œì§ìœ¼ë¡œ ìˆ˜ì •. \n",
    "def clean_data(df):\n",
    "    df_copy = df.copy()\n",
    "    print(\"ğŸ”¥StandardScaler ì‹¤í–‰ì¤‘...\")\n",
    "    scaler = StandardScaler()\n",
    "    amount_n = scaler.fit_transform(df_copy['Amount'].values.reshape(-1, 1))\n",
    "    # ë³€í™˜ëœ Amountë¥¼ Amount_Scaledë¡œ í”¼ì²˜ëª… ë³€ê²½í›„ DataFrameë§¨ ì• ì»¬ëŸ¼ìœ¼ë¡œ ì…ë ¥\n",
    "    df_copy.insert(0, 'Amount_Scaled', amount_n)\n",
    "    # ê¸°ì¡´ Time, Amount í”¼ì²˜ ì‚­ì œ\n",
    "    df_copy.drop(['Time','Amount'], axis=1, inplace=True)\n",
    "    return df_copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ë¦¬ìŠ¤ì¼€ì¼ í›„ ì„±ëŠ¥ ë¹„êµ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Amountë¥¼ ì •ê·œë¶„í¬ í˜•íƒœë¡œ ë³€í™˜ í›„ ë¡œì§€ìŠ¤í‹± íšŒê·€ ë° LightGBM ìˆ˜í–‰. \n",
    "X_train, X_test, y_train, y_test = get_train_test_dataset(data)\n",
    "\n",
    "print('### ë¡œì§€ìŠ¤í‹± íšŒê·€ ì˜ˆì¸¡ ì„±ëŠ¥ ###')\n",
    "lr_clf = LogisticRegression()\n",
    "get_model_train_eval(lr_clf, X_feature_train=X_train, X_feature_test=X_test, Y_feature_train=y_train, Y_feature_test=y_test)\n",
    "\n",
    "print('### LightGBM ì˜ˆì¸¡ ì„±ëŠ¥ ###')\n",
    "lgbm_clf = LGBMClassifier(n_estimators=1000, num_leaves=64, n_jobs=-1, boost_from_average=False)\n",
    "get_model_train_eval(lgbm_clf, X_feature_train=X_train, X_feature_test=X_test, Y_feature_train=y_train, Y_feature_test=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### StandardScaler ì ìš© í›„ ì„±ëŠ¥ ë¹„êµ\n",
    "##### ë¡œì§€ìŠ¤í‹± íšŒê·€ëª¨ë¸ \n",
    "- ì›ë³¸ë°ì´í„° => ì •í™•ë„: 0.9991, ì •ë°€ë„: 0.8491, ì¬í˜„ìœ¨: 0.6081,    â­ï¸F1: 0.7087, AUC:0.9536 \n",
    "- ë¦¬ìŠ¤ì¼€ì¼ ë°ì´í„° => ì •í™•ë„: 0.9992, ì •ë°€ë„: 0.8654, ì¬í˜„ìœ¨: 0.6081,    â­ï¸F1: 0.7143, AUC:0.9702\n",
    "-----------------------\n",
    "##### LightGBM \n",
    "- ì›ë³¸ë°ì´í„° => ì •í™•ë„: 0.9995, ì •ë°€ë„: 0.9573, ì¬í˜„ìœ¨: 0.7568,    â­ï¸F1: 0.8453, AUC:0.9790\n",
    "- ë¦¬ìŠ¤ì¼€ì¼ ë°ì´í„° => ì •í™•ë„: 0.9995, ì •ë°€ë„: 0.9569, ì¬í˜„ìœ¨: 0.7500,    â­ï¸F1: 0.8409, AUC:0.9779\n",
    "\n",
    "#### âœ… ë¦¬ìŠ¤ì¼€ì¼ì„ ì§„í–‰í•œë‹¤ê³  í•´ì„œ ì„±ëŠ¥ì´ ë°˜ë“œì‹œ ì¢‹ì•„ì§€ì§€ ì•ŠìŠµë‹ˆë‹¤.\n",
    "\n",
    "### 7. ì „ì²˜ë¦¬ í•¨ìˆ˜ì— ë¡œê·¸ìŠ¤ì¼€ì¼ ì¶”ê°€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def clean_data(df):\n",
    "    df_copy = df.copy()\n",
    "    print(\"ğŸ”¥ë¡œê·¸ ë³€í™˜ ì‹¤í–‰ì¤‘...\")\n",
    "    # ë„˜íŒŒì´ì˜ log1p( )ë¥¼ ì´ìš©í•˜ì—¬ Amountë¥¼ ë¡œê·¸ ë³€í™˜ \n",
    "    # â­ï¸ log1p()ëŠ” ë„˜íŒŒì´ì˜ log() í•¨ìˆ˜ì™€ ìœ ì‚¬í•˜ë‚˜ log(0)ì€ ë¬´í•œëŒ€ê°€ ë˜ì§€ë§Œ log1p(0)ì€ 0ìœ¼ë¡œ ë°˜í™˜\n",
    "    # â­ï¸â­ï¸ë³µì›ì‹œì—ëŠ” np.expm1()ì„ ì´ìš©í•œë‹¤.\n",
    "    amount_n = np.log1p(df_copy['Amount']) \n",
    "    df_copy.insert(0, 'Amount_Scaled', amount_n)\n",
    "    # ê¸°ì¡´ Time, Amount í”¼ì²˜ ì‚­ì œ\n",
    "    df_copy.drop(['Time','Amount'], axis=1, inplace=True)\n",
    "    return df_copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ë¡œê·¸ë³€í™˜ ë’¤ ì„±ëŠ¥ë¹„êµ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Amountë¥¼ ì •ê·œë¶„í¬ í˜•íƒœë¡œ ë³€í™˜ í›„ ë¡œì§€ìŠ¤í‹± íšŒê·€ ë° LightGBM ìˆ˜í–‰. \n",
    "X_train, X_test, y_train, y_test = get_train_test_dataset(data)\n",
    "\n",
    "print('### ë¡œì§€ìŠ¤í‹± íšŒê·€ ì˜ˆì¸¡ ì„±ëŠ¥ ###')\n",
    "lr_clf = LogisticRegression()\n",
    "get_model_train_eval(lr_clf, X_feature_train=X_train, X_feature_test=X_test, Y_feature_train=y_train, Y_feature_test=y_test)\n",
    "\n",
    "print('### LightGBM ì˜ˆì¸¡ ì„±ëŠ¥ ###')\n",
    "lgbm_clf = LGBMClassifier(n_estimators=1000, num_leaves=64, n_jobs=-1, boost_from_average=False)\n",
    "get_model_train_eval(lgbm_clf, X_feature_train=X_train, X_feature_test=X_test, Y_feature_train=y_train, Y_feature_test=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ë¡œê·¸ë³€í™˜ ì ìš© í›„ ì„±ëŠ¥ ë¹„êµ\n",
    "##### ë¡œì§€ìŠ¤í‹± íšŒê·€ëª¨ë¸ \n",
    "- ì›ë³¸ë°ì´í„° => ì •í™•ë„: 0.9991, ì •ë°€ë„: 0.8491, ì¬í˜„ìœ¨: 0.6081,    â­ï¸F1: 0.7087, AUC:0.9536 \n",
    "- ë¦¬ìŠ¤ì¼€ì¼ ë°ì´í„° => ì •í™•ë„: 0.9992, ì •ë°€ë„: 0.8812, ì¬í˜„ìœ¨: 0.6014,    â­ï¸F1: 0.7149, AUC:0.9727\n",
    "-----------------------\n",
    "##### LightGBM \n",
    "- ì›ë³¸ë°ì´í„° => ì •í™•ë„: 0.9995, ì •ë°€ë„: 0.9573, ì¬í˜„ìœ¨: 0.7568,    â­ï¸F1: 0.8453, AUC:0.9790\n",
    "- ë¦¬ìŠ¤ì¼€ì¼ ë°ì´í„° => ì •í™•ë„: 0.9995, ì •ë°€ë„: 0.9576, ì¬í˜„ìœ¨: 0.7635,    â­ï¸F1: 0.8496, AUC:0.9796\n",
    "\n",
    "#### âœ… ë¦¬ìŠ¤ì¼€ì¼ì„ ì§„í–‰í•œë‹¤ê³  í•´ì„œ ì„±ëŠ¥ì´ ë°˜ë“œì‹œ ì¢‹ì•„ì§€ì§€ ì•Šì§€ë§Œ, ì¡°ê¸ˆ ì¦ê°€í•¨. \n",
    "\n",
    "### 8. Amountì— ë¡œê·¸ë³€í™˜ í›„ ë¶„í¬ ì‹œê°í™”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 4))\n",
    "sns.histplot(X_train['Amount_Scaled'], bins=100, kde=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. ì´ìƒì¹˜ ë°ì´í„° ì œê±° í›„ ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ\n",
    "\n",
    "##### ë¶„ì„ì ˆì°¨\n",
    "1. ë³€ìˆ˜ë“¤ë¼ë¦¬ ìƒê´€ê´€ê³„ ë¶„ì„ ì§„í–‰\n",
    "2. ì¢…ì†ë³€ìˆ˜ì™€ ì—°ê´€ìˆëŠ” ë³€ìˆ˜ ì¶”ì¶œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 15))\n",
    "corr = data.corr()\n",
    "sns.heatmap(corr, cmap='RdBu', annot=True, fmt='.1f')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. ì¢…ì†ë³€ìˆ˜ì™€ ì—°ê´€ìˆëŠ” ë³€ìˆ˜ ì¤‘ V14ë¥¼ ëŒ€ìƒìœ¼ë¡œ ì´ìƒì¹˜ ì°¾ì•„ë³´ì.\n",
    "\n",
    "![IQR_ì´ìƒì¹˜ì œê±°](https://file.notion.so/f/f/4e4750b9-2973-47f2-9fe5-8330ce1f2c19/8fc2f027-1b7a-4259-bf78-88af040b614e/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA_2024-05-16_%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE_10.19.07.png?id=aefab2cc-09a0-4163-85ca-00b77c9892e0&table=block&spaceId=4e4750b9-2973-47f2-9fe5-8330ce1f2c19&expirationTimestamp=1715961600000&signature=LwQiWIxn7twXTnlL6-CKv56PvHWM4lWOq0Uc2BQAfls&downloadName=%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA+2024-05-16+%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE+10.19.07.png)\n",
    "\n",
    "- IQR ë°©ì‹ì€ ê°€ì¥ ë³´í¸ì ì¸ ì´ìƒì¹˜ ì œê±° ë°©ë²•ì…ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['V14'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.percentile(data['V14'].values, 0),\n",
    "       np.percentile(data['V14'].values, 25), \n",
    "       np.percentile(data['V14'].values, 50),\n",
    "       np.percentile(data['V14'].values, 75), \n",
    "       np.percentile(data['V14'].values, 100)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_outlier(df, column, weight=1.5):\n",
    "    # ì‚¬ê¸°ì— í•´ë‹¹í•˜ëŠ” column ë°ì´í„°ë§Œ ì¶”ì¶œ, 1/4 ë¶„ìœ„ì™€ 3/4 ë¶„ìœ„ ì§€ì ì„ np.percentileë¡œ êµ¬í•¨. \n",
    "    fraud = df[df['Class']==1][column]\n",
    "    quantile_25 = np.percentile(fraud.values, 25)\n",
    "    quantile_75 = np.percentile(fraud.values, 75)\n",
    "    # IQRì„ êµ¬í•˜ê³ , IQRì— 1.5ë¥¼ ê³±í•˜ì—¬ ìµœëŒ€ê°’ê³¼ ìµœì†Œê°’ ì§€ì  êµ¬í•¨. \n",
    "    iqr = quantile_75 - quantile_25\n",
    "    iqr_weight = iqr * weight\n",
    "    lowest_val = quantile_25 - iqr_weight\n",
    "    highest_val = quantile_75 + iqr_weight\n",
    "    # ìµœëŒ€ê°’ ë³´ë‹¤ í¬ê±°ë‚˜, ìµœì†Œê°’ ë³´ë‹¤ ì‘ì€ ê°’ì„ ì´ìƒì¹˜ë¡œ ì„¤ì •í•˜ê³  DataFrame index ë°˜í™˜. \n",
    "    outlier_index = fraud[(fraud < lowest_val) | (fraud > highest_val)].index\n",
    "    return outlier_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V14_outliers = get_outlier(data, 'V14', weight=1.5)\n",
    "V14_outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. ë¡œê·¸ë³€í™˜ ì „ì²˜ë¦¬ í•¨ìˆ˜ì— ì´ìƒì¹˜ ì œê±° ë¡œì§ ì¶”ê°€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(df):\n",
    "    df_copy = df.copy()\n",
    "    print(\"ğŸ”¥ë¡œê·¸ ë³€í™˜ ì‹¤í–‰ì¤‘...\")\n",
    "    amount_n = np.log1p(df_copy['Amount']) \n",
    "    df_copy.insert(0, 'Amount_Scaled', amount_n)\n",
    "    df_copy.drop(['Time','Amount'], axis=1, inplace=True)\n",
    "    # ì´ìƒì¹˜ ë°ì´í„° ì‚­ì œí•˜ëŠ” ë¡œì§ ì¶”ê°€\n",
    "    print(\"ğŸ”¥ì´ìƒì¹˜ ì œê±° ì‹¤í–‰ì¤‘...\")\n",
    "    outlier_index = get_outlier(df=df_copy, column='V14', weight=1.5)\n",
    "    df_copy.drop(outlier_index, axis=0, inplace=True)\n",
    "    return df_copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ë¡œê·¸ë³€í™˜ ì ìš© + V14 ì´ìƒì¹˜ ì œê±° ì„±ëŠ¥ ë¹„êµ\n",
    "##### ë¡œì§€ìŠ¤í‹± íšŒê·€ëª¨ë¸ \n",
    "- ì›ë³¸ë°ì´í„°  .  .  .  .  .  .  .  .  .  .  .  .  .  . . => ì •í™•ë„: 0.9991, ì •ë°€ë„: 0.8491, ì¬í˜„ìœ¨: 0.6081,    â­ï¸F1: 0.7087, AUC:0.9536 \n",
    "- ë¦¬ìŠ¤ì¼€ì¼ & ì´ìƒì¹˜ ì œê±° ë°ì´í„° => ì •í™•ë„: 0.9993, ì •ë°€ë„: 0.8750, ì¬í˜„ìœ¨: 0.6712,    â­ï¸F1: 0.7597, AUC:0.9743\n",
    "-----------------------\n",
    "##### LightGBM \n",
    "- ì›ë³¸ë°ì´í„°  .  .  .  .  .  .  .  .  .  .  .  .  .  . . => ì •í™•ë„: 0.9995, ì •ë°€ë„: 0.9573, ì¬í˜„ìœ¨: 0.7568,    â­ï¸F1: 0.8453, AUC:0.9790\n",
    "- ë¦¬ìŠ¤ì¼€ì¼ & ì´ìƒì¹˜ ì œê±° ë°ì´í„° => ì •í™•ë„: 0.9996, ì •ë°€ë„: 0.9603, ì¬í˜„ìœ¨: 0.8288,    â­ï¸F1: 0.8897, AUC:0.9780\n",
    "\n",
    "#### âœ… ë¦¬ìŠ¤ì¼€ì¼ì„ ì§„í–‰í•˜ê³ , ì´ìƒì¹˜ë¥¼ ì§€ìš°ë‹ˆ ì„±ëŠ¥ì´ ë§ì´ ì¢‹ì•„ì¡Œë‹¤!\n",
    "\n",
    "##### â­ï¸ ì´ìƒì¹˜ë¥¼ ì§€ìš¸ë•Œ ìœ ì˜ì‚¬í•­\n",
    "> - 1. ëª¨ë¸ì— ì„±ëŠ¥ì— ì§‘ì¤‘í•´ì„œ ì´ìƒì¹˜ ì œê±°ì— ì¤‘ë…ë˜ëŠ”ë°, ì‚¬ì‹¤ ì´ìƒì¹˜ë¥¼ ëœ ì§€ì›Œì•¼í•˜ëŠ”ê²Œ ì¢‹ìŠµë‹ˆë‹¤. \n",
    "> - 2. í˜„ì‹¤ì—ì„œ ì‹ ê·œ ë°ì´í„° ìœ ì…ì‹œ ì œê±°í–ˆë˜ ê·¸ `ì´ìƒì¹˜ ë°ì´í„°`ê°€ ë“¤ì–´ì˜¬ ìˆ˜ ìˆê¸° ë•Œë¬¸ì…ë‹ˆë‹¤. \n",
    "\n",
    "### 12. SMOTE ì˜¤ë²„ ìƒ˜í”Œë§ í›„ ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%pip install -U imbalanced-learn\n",
    "\n",
    "\n",
    "#### ğŸš¨ ì£¼ì˜ì‚¬í•­\n",
    "1. ëª¨ë¸ `í•™ìŠµ`ë§Œ SMOTE ì‹œí‚¨ ë°ì´í„°ë¥¼ ì‚¬ìš©í•´ì•¼í•©ë‹ˆë‹¤.\n",
    "2. í…ŒìŠ¤íŠ¸ì…‹ì€ ë¯¸ë¦¬ ë¶„ë¦¬í•œ ë°ì´í„°ë¥¼ ì‚¬ìš©í•´ì•¼ë§Œ í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "## 1. SMOTE ì •ì˜\n",
    "smote = SMOTE(random_state=0)\n",
    "## 2. SMOTE ì ìš©\n",
    "X_train_over, y_train_over = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "print('SMOTE ì ìš© ğŸ”´ì „ í•™ìŠµìš© í”¼ì²˜/ë ˆì´ë¸” ë°ì´í„° ì„¸íŠ¸: ', X_train.shape, y_train.shape)\n",
    "print('SMOTE ì ìš© ğŸŸ¢í›„ í•™ìŠµìš© í”¼ì²˜/ë ˆì´ë¸” ë°ì´í„° ì„¸íŠ¸: ', X_train_over.shape, y_train_over.shape)\n",
    "print('SMOTE ì ìš© í›„ ë ˆì´ë¸” ê°’ ë¶„í¬: \\n', pd.Series(y_train_over).value_counts(), '\\n', pd.Series(y_train_over).value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('### SMOTE ë°ì´í„°ì˜ ë¡œì§€ìŠ¤í‹± íšŒê·€ ì˜ˆì¸¡ ì„±ëŠ¥ ###')\n",
    "lr_clf = LogisticRegression(max_iter=1000)\n",
    "get_model_train_eval(lr_clf, \n",
    "                     X_feature_train=X_train_over, \n",
    "                     X_feature_test=X_test, ## âœ… í…ŒìŠ¤íŠ¸ ë°ì´í„°ëŠ” ê·¸ëŒ€ë¡œ ì‚¬ìš©\n",
    "                     Y_feature_train=y_train_over, \n",
    "                     Y_feature_test=y_test ## âœ… í…ŒìŠ¤íŠ¸ ë°ì´í„°ëŠ” ê·¸ëŒ€ë¡œ ì‚¬ìš©\n",
    "                     )\n",
    "\n",
    "print('### SMOTE ë°ì´í„°ì˜ LightGBM ì˜ˆì¸¡ ì„±ëŠ¥ ###')\n",
    "lgbm_clf = LGBMClassifier(n_estimators=1000, num_leaves=64, n_jobs=-1, boost_from_average=False)\n",
    "get_model_train_eval(lgbm_clf, \n",
    "                     X_feature_train=X_train_over, \n",
    "                     X_feature_test=X_test, ## âœ… í…ŒìŠ¤íŠ¸ ë°ì´í„°ëŠ” ê·¸ëŒ€ë¡œ ì‚¬ìš©\n",
    "                     Y_feature_train=y_train_over, \n",
    "                     Y_feature_test=y_test ## âœ… í…ŒìŠ¤íŠ¸ ë°ì´í„°ëŠ” ê·¸ëŒ€ë¡œ ì‚¬ìš©\n",
    "                     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ë¡œê·¸ë³€í™˜ ì ìš© + V14 ì´ìƒì¹˜ ì œê±° + SMOTE ì„±ëŠ¥ ë¹„êµ\n",
    "##### ë¡œì§€ìŠ¤í‹± íšŒê·€ëª¨ë¸ \n",
    "- ì›ë³¸ë°ì´í„°  .  .  .  .  .  .  .  .  .  .  .  .  .  . .  .  . ..  . .  .  . . => ì •í™•ë„: 0.9991, ì •ë°€ë„: 0.8491, ì¬í˜„ìœ¨: 0.6081,    â­ï¸F1: 0.7087, AUC:0.9536 \n",
    "- ë¦¬ìŠ¤ì¼€ì¼ & ì´ìƒì¹˜ ì œê±° & SMOTE ë°ì´í„° => ì •í™•ë„: 0.9723, ì •ë°€ë„: 0.0542, ì¬í˜„ìœ¨: 0.9247,    â­ï¸F1: 0.1023, AUC:0.9737\n",
    "-----------------------\n",
    "##### LightGBM \n",
    "- ì›ë³¸ë°ì´í„°  .  .  .  .  .  .  .  .  .  .  .  .  .  . .  .  . ..  . .  .  . . => ì •í™•ë„: 0.9995, ì •ë°€ë„: 0.9573, ì¬í˜„ìœ¨: 0.7568,    â­ï¸F1: 0.8453, AUC:0.9790\n",
    "- ë¦¬ìŠ¤ì¼€ì¼ & ì´ìƒì¹˜ ì œê±° & SMOTE ë°ì´í„° => ì •í™•ë„: 0.9996, ì •ë°€ë„: 0.9118, ì¬í˜„ìœ¨: 0.8493,    â­ï¸F1: 0.8794, AUC:0.9814\n",
    "\n",
    "- âœ… ë¡œì§€ìŠ¤í‹± íšŒê·€ëª¨ë¸: ì¬í˜„ìœ¨ì€ ë§ì´ ì¢‹ì•„ì¡Œì§€ë§Œ, ë‚˜ë¨¸ì§€ ì„±ëŠ¥ì€ ì•ˆ ì¢‹ì•„ì¡Œë‹¤. SMOTEë¥¼ ì‚¬ìš©í•œë‹¤ê³  í•´ì„œ ë°˜ë“œì‹œ ê°œì„ ë˜ì§€ëŠ” ì•ŠëŠ”ë‹¤.\n",
    "> - ì •ë°€ë„ê°€ ë„ˆë¬´ ë‚®ì•„, ì‚¬ìš©í•˜ê¸° ì–´ë µë‹¤.\n",
    "- âœ… LightGBM : ì¬í˜„ìœ¨ì€ ë§ì´ ì¢‹ì•„ì¡Œê³  F1-scoreë„ ì¢‹ì•„ì¡Œë‹¤. í•˜ì§€ë§Œ ì •ë°€ë„ëŠ” ì¡°ê¸ˆ ë‚˜ë¹ ì¡Œë‹¤. \n",
    "\n",
    "## ì •ë¦¬.\n",
    "\n",
    "- ë¶ˆê· í˜• ë°ì´í„°ì— SMOTEë¥¼ ì ìš©í•˜ë©´ ë°˜ë“œì‹œ ê°œì„ ë˜ëŠ” ê²ƒì€ ì•„ë‹™ë‹ˆë‹¤.\n",
    "- í•˜ì§€ë§Œ, ë¶ˆí¸í•œ ë¹„ìš©ì„ ê°ìˆ˜í•˜ë”ë¼ë„, ì¬í˜„ìœ¨ì„ ë†’ì—¬ì•¼í•˜ëŠ” ì—…ë¬´ì—ëŠ” ì¬í˜„ìœ¨ì´ ë†’ì€ ëª¨ë¸ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "> - ì‹ ìš©ì¹´ë“œ ì‚¬ê¸°, ì•” ì§„ë‹¨, ì¬ë‚œ ê²½ë³´ ì‹œìŠ¤í…œ ë“±"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
